{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import acquire\n",
    "import prepare\n",
    "import model\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is your baseline prediction? What is your baseline accuracy? remember: your baseline prediction for a classification problem is predicting the most prevelant class in the training dataset (the mode). When you make those predictions, what is your accuracy? This is your baseline accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acquire data\n",
    "df = acquire.get_titanic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "train, validate, test = prepare.prep_titanic_data(df, column = 'age', method = 'median', dummies = ['embarked', 'sex'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop out non-numerical columns or non-encoded version remaining in this data set.\n",
    "#in this case, it is only 'passaner_id'\n",
    "train.drop(columns = ['passenger_id'], inplace=True)\n",
    "validate.drop(columns = ['passenger_id'], inplace=True)\n",
    "test.drop(columns = ['passenger_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be attempting to make a Decision Tree Classifier Model that will predict survival on the \n",
    "# Titanic that performs better than the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we are working with survided. The most prevelant class in the training dataset (the mode)\n",
    "train.survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.survived.mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so the baselined is survived = 0\n",
    "train['baseline'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy\n",
    "(train.baseline == train.survived).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The baseline accuracy for nonsurvival in all cases on the Titanic Dataset is {(train.baseline == train.survived).mean():.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report of baseline model\n",
    "baseline_class_report = classification_report(train.survived, train.baseline, zero_division=True)\n",
    "print(baseline_class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #other way to check the accuracy of baseline\n",
    "# #split \n",
    "# X_train, y_train = train.drop(columns='survived'), train.survived\n",
    "# #as the baseline mode is = 0 , thats why we put constant = 0\n",
    "# # 1. Create the object\n",
    "# baseline = DummyClassifier(strategy='constant', constant=0)\n",
    "# # 2. Fit the object\n",
    "# baseline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Baseline accuracy: %.4f' % baseline.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fit the decision tree classifier to your training sample and transform (i.e. make predictions on the training sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove baseline assumption from the train, we wont need this column anymore\n",
    "train.drop(columns='baseline', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the funnction that I create:\n",
    "\n",
    "# X_train, y_train = model.split_Xy(df = train, column = 'survived')\n",
    "# X_validate, y_validate = model.split_Xy(validate, 'survived') \n",
    "# X_test, y_test = model.split_Xy(test, 'survived') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split our X and y\n",
    "# do the capital X, lowercase y thing for train test and split\n",
    "# X is the data frame of the features, y is a series of the target\n",
    "X_train = train.drop(columns='survived')\n",
    "y_train = train[['survived']]\n",
    "X_validate, y_validate = validate.drop(columns='survived'), validate['survived']\n",
    "X_test, y_test = test.drop(columns='survived'), test['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #other way to do the split is decide wich columns we are going to use\n",
    "# X_cols = [ 'pclass','age', 'sex_male','alone', 'embarked_Q', 'embarked_S']\n",
    "# y_col = 'survived'\n",
    "\n",
    "# X_train, y_train = train[X_cols], train[y_col]\n",
    "# X_train, y_train = train[X_cols], train[y_col]\n",
    "# X_validate, y_validate = validate[X_cols], validate[y_col]\n",
    "# X_test, y_test = test[X_cols], test[y_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = DecisionTreeClassifier()\n",
    "# fit the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate the accuracy\n",
    "acc = model.score(X_train, y_train)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# look at the model scores for training set and validate set\n",
    "print(f'training score: {model.score(X_train, y_train):.2%}')\n",
    "print(f'validate score: {model.score(X_validate, y_validate):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(24, 12))\n",
    "plot_tree(\n",
    "    model,\n",
    "    feature_names=X_train.columns.tolist(),\n",
    "    class_names=['died', 'survived'],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as the difference between train and validate is big. I will use  max_features and max_depth\n",
    "#when score.train > score.validate means overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model1 = DecisionTreeClassifier(max_depth=3, max_features=3)\n",
    "# fit the model\n",
    "model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the model scores for training set and validate set\n",
    "print(f'training score: {model1.score(X_train, y_train):.2%}')\n",
    "print(f'validate score: {model1.score(X_validate, y_validate):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(24, 12))\n",
    "plot_tree(\n",
    "    model1,\n",
    "    feature_names=X_train.columns.tolist(),\n",
    "    class_names=['died', 'survived'],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction based on our model1\n",
    "prediction = model1.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(prediction).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "conf_m = confusion_matrix(y_train, prediction )\n",
    "conf_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if rows or columns have the actual number. in this case rows has the actual number\n",
    "train.survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #just to see\n",
    "mat =  pd.DataFrame ((confusion_matrix(y_train, prediction )),index = ['actual_dead','actual_survived'], columns =['pred_dead','pred_survived' ])\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_df = pd.DataFrame([['True Negative', 'False positive'], ['False Negative', 'True Positive']], columns=mat.columns, index=mat.index)\n",
    "rubric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r = rubric_df + ': ' + mat.values.astype(str)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_m[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "positive = survived\n",
    "\n",
    "these calculate the rates\n",
    "- tpr = tp/(tp+fn)\n",
    "- fpr = fp/(fp+tn)\n",
    "- tnr = tn/(tn+fp)\n",
    "- fnr = fn/(fn+tp)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign the values\n",
    "tp = conf_m[1,1]\n",
    "fp =conf_m[0,1] \n",
    "fn= conf_m[1,0]\n",
    "tn =conf_m[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the rate\n",
    "tpr = tp/(tp+fn)\n",
    "fpr = fp/(fp+tn)\n",
    "tnr = tn/(tn+fp)\n",
    "fnr = fn/(fn+tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     matrix = confusion_matrix(y, model.predict(X))\n",
    "#     tpr = matrix[1,1] / (matrix[1,1] + matrix[1,0])\n",
    "#     fpr = matrix[0,1] / (matrix[0,1] + matrix[0,0])\n",
    "#     tnr = matrix[0,0] / (matrix[0,0] + matrix[0,1])\n",
    "#     fnr = matrix[1,0] / (matrix[1,1] + matrix[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the accuracy\n",
    "acc = model1.score(X_train, y_train)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''\n",
    "The accuracy for our model is {acc:.4}\n",
    "The True Positive Rate is {tpr:.3}, The False Positive Rate is {fpr:.3},\n",
    "The True Negative Rate is {tnr:.3}, and the False Negative Rate is {fnr:.3}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#classification report\n",
    "clas_rep =pd.DataFrame(classification_report(y_train, prediction, output_dict=True)).T\n",
    "clas_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(dict(zip(X_train.columns, model1.feature_importances_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model11 = DecisionTreeClassifier(max_depth=3, max_features= 4)\n",
    "model11.fit(X_train, y_train)\n",
    "print(f'training score: {model11.score(X_train, y_train):.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(24, 12))\n",
    "plot_tree(\n",
    "    model11,\n",
    "    feature_names=X_train.columns.tolist(),\n",
    "    class_names=['died', 'survived'],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(dict(zip(X_train.columns, model11.feature_importances_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model2 = DecisionTreeClassifier(max_depth=3)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "print(f'training score: {model2.score(X_train, y_train):.2%}')\n",
    "print(f'validate score: {model2.score(X_validate, y_validate):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we want traning score < validate score.\n",
    "# an aceptable range is a diff of 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(24, 12))\n",
    "plot_tree(\n",
    "    model2,\n",
    "    feature_names=X_train.columns.tolist(),\n",
    "    class_names=['died', 'survived'],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(dict(zip(X_train.columns, model2.feature_importances_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluate your in-sample results using the model score, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****************compute metrics on the TRAIN set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction based on our model\n",
    "prediction = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the values in the predictions. we cannot use value_counts in array thats whay we conver it to pd.series\n",
    "pd.Series(prediction).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model Score: Accuracy\n",
    "\n",
    "acc = model1.score(X_train, y_train)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the actual numbers\n",
    "train.survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if you change the order of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, prediction, labels=(1, 0) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, prediction, labels=(0, 1) )#this is the default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the rows are the actual dead  because 0 + 307 = 307, \n",
    "confusion_matrix(y_train, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm= pd.DataFrame ((confusion_matrix(y_train, prediction )),index = ['actual_dead','actual_survived'], columns =['pred_dead','pred_survived' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model Score: Accuracy\n",
    "\n",
    "acc = model1.score(X_train, y_train)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(classification_report(y_train, prediction, output_dict=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compute: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### on the validate set *****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- positive is : survived\n",
    "- True Positive: predict survived and it is survived\n",
    "- True Negative: predict death and it is death\n",
    "- False Positive: predict survived and it is death\n",
    "- False Negative: predict death and it is survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the model score on the validate set\n",
    "print(f'validate score: {model1.score(X_validate, y_validate):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = model1.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate.survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix(validate.survived, val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2= confusion_matrix(y_validate, val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just to see\n",
    "pd.DataFrame (confusion_matrix(y_validate, val_pred),index = ['actual_death','actual_survived'], columns =['pred_death','pred_survived' ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#psitive : survived\n",
    "#assign the values\n",
    "tp = cm2[1,1]\n",
    "fp =cm2[0,1] \n",
    "fn= cm2[1,0]\n",
    "tn =cm2[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the rate\n",
    "tpr = tp/(tp+fn)\n",
    "fpr = fp/(fp+tn)\n",
    "tnr = tn/(tn+fp)\n",
    "fnr = fn/(fn+tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''\n",
    "\n",
    "The True Positive Rate is {tpr:.3}, The False Positive Rate is {fpr:.3},\n",
    "The True Negative Rate is {tnr:.3}, and the False Negative Rate is {fnr:.3}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy\n",
    "acc_model1 =accuracy_score(y_validate, val_pred)\n",
    "acc_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision\n",
    "prec_model1= precision_score(y_validate, val_pred, pos_label = 1 )\n",
    "prec_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recall\n",
    "rec_model1= recall_score(y_validate, val_pred, pos_label = 1 )\n",
    "rec_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(classification_report(y_validate, val_pred, output_dict= True))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'0': \"dead\", '1': \"survived\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run through steps 2-4 using a different max_depth value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model2 = DecisionTreeClassifier(max_depth=2)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "print(f'training score: {model2.score(X_train, y_train):.2%}')\n",
    "print(f'validate score: {model2.score(X_validate, y_validate):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(dict(zip(X_train.columns, model2.feature_importances_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#using my function import model \n",
    "model.model_performs(X_train, y_train, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if I want to see a dec_tree\n",
    "model.dec_tree(model2, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model3 = DecisionTreeClassifier(max_depth= 5)\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "print(f'training score: {model3.score(X_train, y_train):.2%}')\n",
    "print(f'validate score: {model3.score(X_validate, y_validate):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#using my function import model \n",
    "model.model_performs(X_train, y_train, model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if I want to see a dec_tree\n",
    "model.dec_tree(model3, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy\n",
    "acc_model2 =accuracy_score(train.survived, train.prediction_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision\n",
    "prec_model2 =precision_score(train.survived, train.prediction_2, pos_label = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recall\n",
    "rec_model2= recall_score(train.survived, train.prediction_2, pos_label = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Which model performs better on your in-sample data? (aka TRAIN set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 using my func\n",
    "model1 = DecisionTreeClassifier(max_depth=3, max_features=3)\n",
    "\n",
    "model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model_performs(X_train, y_train, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if I want to see a dec_tree\n",
    "model.dec_tree(model1, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.Which model performs best on your out-of-sample data, the validate set? (aka the VALIDATE set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for our validation sets\n",
    "val_pred_1 = model1.predict(X_validate)\n",
    "val_pred_2 = model2.predict(X_validate)\n",
    "val_pred_3 = model3.predict(X_validate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get validation accuracy\n",
    "acc_v_1 = model1.score(X_validate, y_validate)\n",
    "acc_v_2 = model2.score(X_validate, y_validate)\n",
    "acc_v_3 = model3.score(X_validate, y_validate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_v_1, acc_v_2, acc_v_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the model scores for training set and validate set\n",
    "print(f'training score: {model1.score(X_train, y_train):.2%}')\n",
    "print(f'validate score: {model1.score(X_validate, y_validate):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# look at the model scores for training set and validate set\n",
    "print(f'training score: {model1.score(X_train, y_train):.2%}')\n",
    "print(f'validate score: {model1.score(X_validate, y_validate):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# look at the model scores for training set and validate set\n",
    "print(f'training score: {model2.score(X_train, y_train):.2%}')\n",
    "print(f'validate score: {model2.score(X_validate, y_validate):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look at the model scores for training set and validate set\n",
    "print(f'training score: {model3.score(X_train, y_train):.2%}')\n",
    "print(f'validate score: {model3.score(X_validate, y_validate):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ":\n",
    "# \n",
    "# \n",
    "# We see here a very significant drop-off on model ,\n",
    "# where we did not specify a maximum depth, \n",
    "# suggesting it was very overfit to the training set\n",
    "# training score >>> validate score = overfit\n",
    "# b\n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(model1, feature_names= X_train.columns, rounded=True, filled=True, out_file=None)\n",
    "graph = graphviz.Source(dot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.render('titanic_model_1_tree', view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(model3, feature_names= X_train.columns, rounded=True, filled=True, out_file=None)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render('titanic_model_1_tree', view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fit the Random Forest classifier to your training sample and transform (i.e. make predictions on the training sample) setting the random_state accordingly and setting min_samples_leaf = 1 and max_depth = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acquire data\n",
    "df = acquire.get_titanic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "train, validate, test = prepare.prep_titanic_data(df, column = 'age', method = 'median', dummies = ['embarked', 'sex'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop out non-numerical columns or non-encoded version remaining in this data set.\n",
    "#in this case, it is only 'passaner_id'\n",
    "train.drop(columns = ['passenger_id'], inplace=True)\n",
    "validate.drop(columns = ['passenger_id'], inplace=True)\n",
    "test.drop(columns = ['passenger_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the funnction that I create:\n",
    "\n",
    "X_train, y_train = model.split_Xy(df = train, column = 'survived')\n",
    "X_validate, y_validate = model.split_Xy(validate, 'survived') \n",
    "X_test, y_test = model.split_Xy(test, 'survived') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived    0.616466\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish our baseline.  The rate at which the assumption of the majority class matches the real values.  \n",
    "#If a model does not perform better than this, it would not be wise to deploy.\n",
    "baseline = (y_train.value_counts().idxmax() == y_train).mean()\n",
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Random Forest Model\n",
    "rfmodel = RandomForestClassifier(min_samples_leaf=1, max_depth=10, random_state=1349)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=10, random_state=1349)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the thing\n",
    "rfmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the next step but I already have it in my function\n",
    "#  use the thing\n",
    "# y_pred = rfmodel.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluate your results using the model score, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The accuracy for our model is 96.9880%\n",
      "\n",
      "    The True Positive Rate is 92.147%,    The False Positive Rate is 0.000%,\n",
      "    The True Negative Rate is 100.000%,    The False Negative Rate is 7.853%\n",
      "\n",
      "    ________________________________________________________________________________\n",
      "    \n",
      "\n",
      "    The positive is  'survived'\n",
      "\n",
      "    Confusion Matrix\n",
      "\n",
      "                               pred_dead       pred_survived\n",
      "actual_dead      True Negative: 307   False positive: 0\n",
      "actual_survived  False Negative: 15  True Positive: 176\n",
      "\n",
      "\n",
      "    ________________________________________________________________________________\n",
      "    \n",
      "    Classification Report:\n",
      "    \n",
      "                   precision    recall  f1-score    support\n",
      "dead           0.953416  1.000000  0.976153  307.00000\n",
      "survived       1.000000  0.921466  0.959128  191.00000\n",
      "accuracy       0.969880  0.969880  0.969880    0.96988\n",
      "macro avg      0.976708  0.960733  0.967640  498.00000\n",
      "weighted avg   0.971283  0.969880  0.969623  498.00000\n"
     ]
    }
   ],
   "source": [
    "model.model_performs(X_train, y_train, rfmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The accuracy for our model is 96.9880%\n",
      "\n",
      "    The True Positive Rate is 92.147%,    The False Positive Rate is 0.000%,\n",
      "    The True Negative Rate is 100.000%,    The False Negative Rate is 7.853%\n",
      "\n",
      "    ________________________________________________________________________________\n",
      "    \n",
      "\n",
      "    The positive is  'survived'\n",
      "\n",
      "    Confusion Matrix\n",
      "\n",
      "                               pred_dead       pred_survived\n",
      "actual_dead      True Negative: 307   False positive: 0\n",
      "actual_survived  False Negative: 15  True Positive: 176\n",
      "\n",
      "\n",
      "    ________________________________________________________________________________\n",
      "    \n",
      "    Classification Report:\n",
      "    \n",
      "                   precision    recall  f1-score    support\n",
      "dead           0.953416  1.000000  0.976153  307.00000\n",
      "survived       1.000000  0.921466  0.959128  191.00000\n",
      "accuracy       0.969880  0.969880  0.969880    0.96988\n",
      "macro avg      0.976708  0.960733  0.967640  498.00000\n",
      "weighted avg   0.971283  0.969880  0.969623  498.00000\n"
     ]
    }
   ],
   "source": [
    "model.model_performs(X_train, y_train, rfmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run through steps increasing your min_samples_leaf and decreasing your max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=4, min_samples_leaf=4, random_state=1349)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the Random Forest Model\n",
    "rfmod2 = RandomForestClassifier(min_samples_leaf=4, max_depth=4, random_state=1349)\n",
    "\n",
    "# fit the model\n",
    "rfmod2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The accuracy for our model is 85.1406%\n",
      "\n",
      "    The True Positive Rate is 70.157%,    The False Positive Rate is 5.537%,\n",
      "    The True Negative Rate is 94.463%,    The False Negative Rate is 29.843%\n",
      "\n",
      "    ________________________________________________________________________________\n",
      "    \n",
      "\n",
      "    The positive is  'survived'\n",
      "\n",
      "    Confusion Matrix\n",
      "\n",
      "                               pred_dead       pred_survived\n",
      "actual_dead      True Negative: 290  False positive: 17\n",
      "actual_survived  False Negative: 57  True Positive: 134\n",
      "\n",
      "\n",
      "    ________________________________________________________________________________\n",
      "    \n",
      "    Classification Report:\n",
      "    \n",
      "                   precision    recall  f1-score     support\n",
      "dead           0.835735  0.944625  0.886850  307.000000\n",
      "survived       0.887417  0.701571  0.783626  191.000000\n",
      "accuracy       0.851406  0.851406  0.851406    0.851406\n",
      "macro avg      0.861576  0.823098  0.835238  498.000000\n",
      "weighted avg   0.855557  0.851406  0.847260  498.000000\n"
     ]
    }
   ],
   "source": [
    "model.model_performs(X_train, y_train, rfmod2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What are the differences in the evaluation metrics? Which performs better on your in-sample data? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The accuracy for our model is 82.2430%\n",
      "\n",
      "    The True Positive Rate is 71.951%,    The False Positive Rate is 11.364%,\n",
      "    The True Negative Rate is 88.636%,    The False Negative Rate is 28.049%\n",
      "\n",
      "    ________________________________________________________________________________\n",
      "    \n",
      "\n",
      "    The positive is  'survived'\n",
      "\n",
      "    Confusion Matrix\n",
      "\n",
      "                               pred_dead       pred_survived\n",
      "actual_dead      True Negative: 117  False positive: 15\n",
      "actual_survived  False Negative: 23   True Positive: 59\n",
      "\n",
      "\n",
      "    ________________________________________________________________________________\n",
      "    \n",
      "    Classification Report:\n",
      "    \n",
      "                   precision    recall  f1-score    support\n",
      "dead           0.835714  0.886364  0.860294  132.00000\n",
      "survived       0.797297  0.719512  0.756410   82.00000\n",
      "accuracy       0.822430  0.822430  0.822430    0.82243\n",
      "macro avg      0.816506  0.802938  0.808352  214.00000\n",
      "weighted avg   0.820994  0.822430  0.820488  214.00000\n"
     ]
    }
   ],
   "source": [
    "model.model_performs(X_validate, y_validate, rfmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.model_performs(X_validate, y_validate, rfmod2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After making a few models, which one has the best performance (or closest metrics) on both train and validate?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
